Empirical_CDF_values_dataset_01 = noquote(substr(ecdf_values_1st_dataset_only,1,5)),
Empirical_CDF_values_dataset_02 = noquote(substr(ecdf_values_2nd_dataset_only,1,5)),
Difference_in_ECDF_values = round(difference_in_ecdf_values,3) )
## print it
print("------------------------------------------------------")
print(out,row.names=F)
print("------------------------------------------------------")
## compute the test statistic
## T = entry in 5th column with maximum magnitude (ignore sign)
test_stat3 = max(abs(difference_in_ecdf_values))
print("test statistics")
print(round(test_stat3,3))
knitr::opts_chunk$set(echo = TRUE)
# read in file Assignment_02_data_01.txt
dataset = as.matrix(read.table("Assignment_02_data_01.txt", header = T))
n = length(dataset) ## size of dataset
print("sample size")
print(n)
mu_hat = mean(dataset) ## estimate the mu parameter from data
sigma2_hat = var(dataset) ## estimate the sigma^2 parameter from data
lambda_hat = 1/mean(dataset) ## estimate the lambda parameter from data
print(paste("Fitting a N(",round(mu_hat,3),",",round(sigma2_hat,3),") parametric model",sep=""))
## specify starting integer of each cell, separated by commas
I_start = c(-Inf, 16, 17.2, 17.9, 18.4, 19, 19.8, 20.4, 21, 21.7)
## specify ending integer of each cell, separated by commas
I_end = c(16, 17.2, 17.9, 18.4, 19, 19.8, 20.4, 21, 21.7, Inf)
k = length(I_start) ## how many cells there are
## calculate the number of observations in each cell
n_freq = array(0,k)
for (i in 1:k)
n_freq[i] = length(which( dataset > I_start[i] & dataset <= I_end[i] ))
p_cell = array(0,k)
## run the following four lines only for Normal distribution, skip them otherwise
print(paste("Fitting a N(",round(mu_hat,3),",",round(sigma2_hat,3),") parametric model",sep=""))
for (i in 1:k)
p_cell[i] = pnorm(I_end[i], mean = mu_hat, sd = sqrt(sigma2_hat)) -
pnorm(I_start[i], mean = mu_hat, sd = sqrt(sigma2_hat))
####################################################################
## calculate the discrepancy column
discrepancy = n_freq - n*p_cell
## now, create the table
## create the first column where the cells are specified
cells = array(0,k)
for (i in 1:k) {
cells[i] = paste("(",I_start[i], ",", I_end[i],"]",sep="")
}
out = data.frame( Cells = noquote(cells),
data_count = n_freq,
parametric_probability = round(p_cell,3),
discrepancy = round(discrepancy,3) )
## print it
print("------------------------------------------------------")
print(out,row.names=F)
print("------------------------------------------------------")
## check required conditions
print("check required conditions:")
print(sum(out$data_count)) ## 2nd required condition
print(sum(out$parametric_probability)) ## 3rd required condition
print(sum(out$discrepancy)) ## 4th required condition
## check recommended condition
print("Check recommended condition")
## reconstruct the cells if any of the following entries is below 5
print(n*p_cell)
test_stat <- sum((out$discrepancy^2)/(n*out$parametric_probability))
print(test_stat)
# read in file Assignment_02_data_02.txt
dataset2 = as.matrix(read.table("Assignment_02_data_02.txt", header = T))
n2 = length(dataset2) ## size of dataset
print("sample size")
print(n2)
mu_hat = mean(dataset2) ## sample mean
sigma2_hat = var(dataset2) ## sample variance
print("sample mean:")
print(mu_hat)
print("sample variance:")
print(sigma2_hat)
## first column: sorted unique values
sorted_unique_values = sort(unique(dataset2))
k = length(sorted_unique_values) ## how many unique values
## second column: normal cdf at those k values
normal_cdf_values = array(0,k)
for (i in 1:k)
normal_cdf_values[i] = pnorm(sorted_unique_values[i],mean=mu_hat, sd = sqrt(sigma2_hat))
## third column: empirical cdf at those k values
## empirical CDF function
empirical_CDF = ecdf(dataset2)
empirical_cdf_values = array(0,k)
for (i in 1:k)
empirical_cdf_values[i] = empirical_CDF(sorted_unique_values[i])
## fourth column: Normal CDF (at any value) - empirical CDF (at that value)
difference_1st_kind = normal_cdf_values - empirical_cdf_values
## fifth column: empirical cdf at previous values
empirical_cdf_previous_values = array(0,k)
## 1st position is always = 0, start from 2nd position
for (i in 2:k)
empirical_cdf_previous_values[i] = empirical_cdf_values[i-1]
## sixth column: Normal CDF (at any value) - empirical CDF (at previous value)
difference_2nd_kind = normal_cdf_values - empirical_cdf_previous_values
## now, create the table
out = data.frame( sorted_Unique_observations = sorted_unique_values,
Normal_CDF_value = round(normal_cdf_values,3),
Empirical_CDF_value = round(empirical_cdf_values,3),
Discrepancy_1st_kind = round(difference_1st_kind,3),
Empirical_CDF_previous_value = round(empirical_cdf_previous_values,3),
Discrepancy_2nd_kind = round(difference_2nd_kind,3) )
## print it
print("------------------------------------------------------")
print(out,row.names=F)
print("------------------------------------------------------")
test_stat2 = max(abs(c(difference_1st_kind,difference_2nd_kind)))
print("test statistics")
print(round(test_stat2,3))
# read in file Assignment_02_data_03.txt
dataset3 = as.matrix(read.table("Assignment_02_data_03.txt", header = T))
n3 = length(dataset3) ## size of dataset
print("sample sizes:")
print(c(n2,n3))
## first, pool the data without sorting
data_pooled = c(dataset2,dataset3)
## since we have not sorted them yet, currently 1st n1 many positions
## correspond to dataset 1, next n2 many positions correspond to dataset 2
unsorted_dataset_indicator = c(rep(1,n2),rep(2,n3))
## now sort the pooled data from smallest to largest
data_pooled_sorted = sort(data_pooled) ## 1st column construction
## rearrange the indicator column depending on the order of the order of the pooled data
sorted_dataset_indicator = unsorted_dataset_indicator[order(data_pooled)]
## 2nd column is constructed
## now compute the empirical cdf function for each dataset
empirical_CDF_01 = ecdf(dataset2)
empirical_CDF_02 = ecdf(dataset3)
## third column: ecdf values for observations from 1st dataset
ecdf_values_1st_dataset_only = array("",n2+n3)
for (i in 1:n2)
ecdf_values_1st_dataset_only[which(sorted_dataset_indicator==1)[i]] =
empirical_CDF_01(data_pooled_sorted[which(sorted_dataset_indicator==1)[i]])
## fourth column: ecdf values for observations from 2nd dataset
ecdf_values_2nd_dataset_only = array("",n2+n3)
for (i in 1:n3)
ecdf_values_2nd_dataset_only[which(sorted_dataset_indicator==2)[i]] =
empirical_CDF_02(data_pooled_sorted[which(sorted_dataset_indicator==2)[i]])
## fifth column: difference between ecdf for dataset 2 and ecdf for dataset 3 at all values
difference_in_ecdf_values = array(0,n2+n3)
for (i in 1:(n2+n3))
difference_in_ecdf_values[i] =
empirical_CDF_01(data_pooled_sorted[i]) - empirical_CDF_02(data_pooled_sorted[i])
## now, create the table
out = data.frame( sorted_pooled_observations = data_pooled_sorted,
dataset_indicator = sorted_dataset_indicator,
Empirical_CDF_values_dataset_01 = noquote(substr(ecdf_values_1st_dataset_only,1,5)),
Empirical_CDF_values_dataset_02 = noquote(substr(ecdf_values_2nd_dataset_only,1,5)),
Difference_in_ECDF_values = round(difference_in_ecdf_values,3) )
## print it
print("------------------------------------------------------")
print(out,row.names=F)
print("------------------------------------------------------")
## compute the test statistic
## T = entry in 5th column with maximum magnitude (ignore sign)
test_stat3 = max(abs(difference_in_ecdf_values))
print("test statistics")
print(round(test_stat3,3))
knitr::opts_chunk$set(echo = TRUE)
# read in file Assignment_02_data_01.txt
dataset = as.matrix(read.table("Assignment_02_data_01.txt", header = T))
n = length(dataset) ## size of dataset
print("sample size")
print(n)
mu_hat = mean(dataset) ## estimate the mu parameter from data
sigma2_hat = var(dataset) ## estimate the sigma^2 parameter from data
lambda_hat = 1/mean(dataset) ## estimate the lambda parameter from data
print(paste("Fitting a N(",round(mu_hat,3),",",round(sigma2_hat,3),") parametric model",sep=""))
## specify starting integer of each cell, separated by commas
I_start = c(-Inf, 16, 17.2, 17.9, 18.4, 19, 19.8, 20.4, 21, 21.7)
## specify ending integer of each cell, separated by commas
I_end = c(16, 17.2, 17.9, 18.4, 19, 19.8, 20.4, 21, 21.7, Inf)
k = length(I_start) ## how many cells there are
## calculate the number of observations in each cell
n_freq = array(0,k)
for (i in 1:k)
n_freq[i] = length(which( dataset > I_start[i] & dataset <= I_end[i] ))
p_cell = array(0,k)
## run the following four lines only for Normal distribution, skip them otherwise
print(paste("Fitting a N(",round(mu_hat,3),",",round(sigma2_hat,3),") parametric model",sep=""))
for (i in 1:k)
p_cell[i] = pnorm(I_end[i], mean = mu_hat, sd = sqrt(sigma2_hat)) -
pnorm(I_start[i], mean = mu_hat, sd = sqrt(sigma2_hat))
####################################################################
## calculate the discrepancy column
discrepancy = n_freq - n*p_cell
## now, create the table
## create the first column where the cells are specified
cells = array(0,k)
for (i in 1:k) {
cells[i] = paste("(",I_start[i], ",", I_end[i],"]",sep="")
}
out = data.frame( Cells = noquote(cells),
data_count = n_freq,
parametric_probability = round(p_cell,3),
discrepancy = discrepancy )
## print it
print("------------------------------------------------------")
print(out,row.names=F)
print("------------------------------------------------------")
## check required conditions
print("check required conditions:")
print(sum(out$data_count)) ## 2nd required condition
print(sum(out$parametric_probability)) ## 3rd required condition
print(sum(out$discrepancy)) ## 4th required condition
## check recommended condition
print("Check recommended condition")
## reconstruct the cells if any of the following entries is below 5
print(n*p_cell)
test_stat <- sum((out$discrepancy^2)/(n*out$parametric_probability))
print(test_stat)
# read in file Assignment_02_data_02.txt
dataset2 = as.matrix(read.table("Assignment_02_data_02.txt", header = T))
n2 = length(dataset2) ## size of dataset
print("sample size")
print(n2)
mu_hat = mean(dataset2) ## sample mean
sigma2_hat = var(dataset2) ## sample variance
print("sample mean:")
print(mu_hat)
print("sample variance:")
print(sigma2_hat)
## first column: sorted unique values
sorted_unique_values = sort(unique(dataset2))
k = length(sorted_unique_values) ## how many unique values
## second column: normal cdf at those k values
normal_cdf_values = array(0,k)
for (i in 1:k)
normal_cdf_values[i] = pnorm(sorted_unique_values[i],mean=mu_hat, sd = sqrt(sigma2_hat))
## third column: empirical cdf at those k values
## empirical CDF function
empirical_CDF = ecdf(dataset2)
empirical_cdf_values = array(0,k)
for (i in 1:k)
empirical_cdf_values[i] = empirical_CDF(sorted_unique_values[i])
## fourth column: Normal CDF (at any value) - empirical CDF (at that value)
difference_1st_kind = normal_cdf_values - empirical_cdf_values
## fifth column: empirical cdf at previous values
empirical_cdf_previous_values = array(0,k)
## 1st position is always = 0, start from 2nd position
for (i in 2:k)
empirical_cdf_previous_values[i] = empirical_cdf_values[i-1]
## sixth column: Normal CDF (at any value) - empirical CDF (at previous value)
difference_2nd_kind = normal_cdf_values - empirical_cdf_previous_values
## now, create the table
out = data.frame( sorted_Unique_observations = sorted_unique_values,
Normal_CDF_value = round(normal_cdf_values,3),
Empirical_CDF_value = round(empirical_cdf_values,3),
Discrepancy_1st_kind = round(difference_1st_kind,3),
Empirical_CDF_previous_value = round(empirical_cdf_previous_values,3),
Discrepancy_2nd_kind = round(difference_2nd_kind,3) )
## print it
print("------------------------------------------------------")
print(out,row.names=F)
print("------------------------------------------------------")
test_stat2 = max(abs(c(difference_1st_kind,difference_2nd_kind)))
print("test statistics")
print(round(test_stat2,3))
# read in file Assignment_02_data_03.txt
dataset3 = as.matrix(read.table("Assignment_02_data_03.txt", header = T))
n3 = length(dataset3) ## size of dataset
print("sample sizes:")
print(c(n2,n3))
## first, pool the data without sorting
data_pooled = c(dataset2,dataset3)
## since we have not sorted them yet, currently 1st n1 many positions
## correspond to dataset 1, next n2 many positions correspond to dataset 2
unsorted_dataset_indicator = c(rep(1,n2),rep(2,n3))
## now sort the pooled data from smallest to largest
data_pooled_sorted = sort(data_pooled) ## 1st column construction
## rearrange the indicator column depending on the order of the order of the pooled data
sorted_dataset_indicator = unsorted_dataset_indicator[order(data_pooled)]
## 2nd column is constructed
## now compute the empirical cdf function for each dataset
empirical_CDF_01 = ecdf(dataset2)
empirical_CDF_02 = ecdf(dataset3)
## third column: ecdf values for observations from 1st dataset
ecdf_values_1st_dataset_only = array("",n2+n3)
for (i in 1:n2)
ecdf_values_1st_dataset_only[which(sorted_dataset_indicator==1)[i]] =
empirical_CDF_01(data_pooled_sorted[which(sorted_dataset_indicator==1)[i]])
## fourth column: ecdf values for observations from 2nd dataset
ecdf_values_2nd_dataset_only = array("",n2+n3)
for (i in 1:n3)
ecdf_values_2nd_dataset_only[which(sorted_dataset_indicator==2)[i]] =
empirical_CDF_02(data_pooled_sorted[which(sorted_dataset_indicator==2)[i]])
## fifth column: difference between ecdf for dataset 2 and ecdf for dataset 3 at all values
difference_in_ecdf_values = array(0,n2+n3)
for (i in 1:(n2+n3))
difference_in_ecdf_values[i] =
empirical_CDF_01(data_pooled_sorted[i]) - empirical_CDF_02(data_pooled_sorted[i])
## now, create the table
out = data.frame( sorted_pooled_observations = data_pooled_sorted,
dataset_indicator = sorted_dataset_indicator,
Empirical_CDF_values_dataset_01 = noquote(substr(ecdf_values_1st_dataset_only,1,5)),
Empirical_CDF_values_dataset_02 = noquote(substr(ecdf_values_2nd_dataset_only,1,5)),
Difference_in_ECDF_values = round(difference_in_ecdf_values,3) )
## print it
print("------------------------------------------------------")
print(out,row.names=F)
print("------------------------------------------------------")
## compute the test statistic
## T = entry in 5th column with maximum magnitude (ignore sign)
test_stat3 = max(abs(difference_in_ecdf_values))
print("test statistics")
print(round(test_stat3,3))
knitr::opts_chunk$set(echo = TRUE)
ark_schedule <- gamezoneR::gamezone_mbb_team_schedule(team = "Arkansas", season = "2021-22")
ark_schedule <- gamezoneR::gamezone_mbb_team_schedule(team = "Arkansas", season = "2021-22")
uk_schedule <- gamezoneR::gamezone_mbb_team_schedule(team = "Kentucky", season = "2021-22")
ala_schedule <- gamezoneR::gamezone_mbb_team_schedule(team = "Alabama", season = "2021-22")
lsu_schedule <- gamezoneR::gamezone_mbb_team_schedule(team = "LSU", season = "2021-22")
tamu_schedule <- gamezoneR::gamezone_mbb_team_schedule(team = "Texas A&M", season = "2021-22")
mst_schedule <- gamezoneR::gamezone_mbb_team_schedule(team = "Mississippi State", season = "2021-22")
miss_schedule <- gamezoneR::gamezone_mbb_team_schedule(team = "Ole Miss", season = "2021-22")
sc_schedule <- gamezoneR::gamezone_mbb_team_schedule(team = "South Carolina", season = "2021-22")
uga_schedule <- gamezoneR::gamezone_mbb_team_schedule(team = "Georgia", season = "2021-22")
uf_schedule <- gamezoneR::gamezone_mbb_team_schedule(team = "Florida", season = "2021-22")
vand_schedule <- gamezoneR::gamezone_mbb_team_schedule(team = "Vanderbilt", season = "2021-22")
aub_schedule <- gamezoneR::gamezone_mbb_team_schedule(team = "Auburn", season = "2021-22")
mizz_schedule <- gamezoneR::gamezone_mbb_team_schedule(team = "Missouri", season = "2021-22")
tenn_schedule <- gamezoneR::gamezone_mbb_team_schedule(team = "Tennessee", season = "2021-22")
ark_pbp <- purrr::map_df(ark_schedule$game_id,
gamezoneR::gamezone_mbb_pbp, sub_parse = F)
ark_pbp %>%
dplyr::filter(!is.na(poss_before)) %>%
dplyr::mutate(poss_number = as.numeric(poss_number),
shot_made_numeric = dplyr::case_when(
is.na(shot_outcome) ~ NA_real_,
shot_outcome == "made" ~ 1,
shot_outcome == "missed" ~ 0),
shot_value = dplyr::case_when(
is.na(shot_outcome) ~ NA_real_,
free_throw == 1 ~ 1,
three_pt == 1 ~ 3,
T ~ 2),
points = dplyr::case_when(
shot_made_numeric == 0 ~ 0,
shot_made_numeric == 1 & free_throw == 1 ~ 1,
shot_made_numeric == 1 & three_pt == 1 ~ 3,
shot_made_numeric == 1 & three_pt == 0 & free_throw == 0 ~ 2)) %>%
dplyr::group_by(date, game_id, poss_before, poss_number) %>%
dplyr::summarise(fgm = sum(shot_outcome == "made" & free_throw == F, na.rm = T),
fga = sum(!is.na(shot_outcome) & free_throw == F),
ftm = sum(shot_outcome == "made" & free_throw == T),
fta = sum(!is.na(shot_outcome) & free_throw == T),
points = sum(points, na.rm = T),
.groups = "drop") %>%
dplyr::group_by(date, game_id, poss_before) %>%
dplyr::summarise(poss = dplyr::n(),
across(fgm:points, sum),
.groups = "drop") %>%
dplyr::mutate(ppp = points/poss,
type = ifelse(poss_before == "Arkansas", "Offense", "Defense"),
color = ifelse(poss_before == "Arkansas", "black", "#003366"),
fill = ifelse(poss_before == "Arkansas", "#001A57", "white")) %>%
ggplot(aes(date, ppp, fill = fill, color = color)) +
geom_line() +
geom_point(aes(size = poss),
pch = 21, stroke = 0.9) +
scale_color_identity() +
scale_fill_identity() +
scale_size_continuous(range = c(0.8, 3.5)) +
labs(title = "Arkansas' offensive and defensive efficiency by game",
subtitle = "2018-19 college basketball season",
x = "Date",
y = "Points per possession",
size = "# of possessions",
caption = "Chart: @jacobhaarala | Data: @gamezoneR")
library(dplyr)
ark_pbp %>%
dplyr::filter(!is.na(poss_before)) %>%
dplyr::mutate(poss_number = as.numeric(poss_number),
shot_made_numeric = dplyr::case_when(
is.na(shot_outcome) ~ NA_real_,
shot_outcome == "made" ~ 1,
shot_outcome == "missed" ~ 0),
shot_value = dplyr::case_when(
is.na(shot_outcome) ~ NA_real_,
free_throw == 1 ~ 1,
three_pt == 1 ~ 3,
T ~ 2),
points = dplyr::case_when(
shot_made_numeric == 0 ~ 0,
shot_made_numeric == 1 & free_throw == 1 ~ 1,
shot_made_numeric == 1 & three_pt == 1 ~ 3,
shot_made_numeric == 1 & three_pt == 0 & free_throw == 0 ~ 2)) %>%
dplyr::group_by(date, game_id, poss_before, poss_number) %>%
dplyr::summarise(fgm = sum(shot_outcome == "made" & free_throw == F, na.rm = T),
fga = sum(!is.na(shot_outcome) & free_throw == F),
ftm = sum(shot_outcome == "made" & free_throw == T),
fta = sum(!is.na(shot_outcome) & free_throw == T),
points = sum(points, na.rm = T),
.groups = "drop") %>%
dplyr::group_by(date, game_id, poss_before) %>%
dplyr::summarise(poss = dplyr::n(),
across(fgm:points, sum),
.groups = "drop") %>%
dplyr::mutate(ppp = points/poss,
type = ifelse(poss_before == "Arkansas", "Offense", "Defense"),
color = ifelse(poss_before == "Arkansas", "black", "#003366"),
fill = ifelse(poss_before == "Arkansas", "#001A57", "white")) %>%
ggplot(aes(date, ppp, fill = fill, color = color)) +
geom_line() +
geom_point(aes(size = poss),
pch = 21, stroke = 0.9) +
scale_color_identity() +
scale_fill_identity() +
scale_size_continuous(range = c(0.8, 3.5)) +
labs(title = "Arkansas' offensive and defensive efficiency by game",
subtitle = "2018-19 college basketball season",
x = "Date",
y = "Points per possession",
size = "# of possessions",
caption = "Chart: @jacobhaarala | Data: @gamezoneR")
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
ark_pbp %>%
dplyr::filter(!is.na(poss_before)) %>%
dplyr::mutate(poss_number = as.numeric(poss_number),
shot_made_numeric = dplyr::case_when(
is.na(shot_outcome) ~ NA_real_,
shot_outcome == "made" ~ 1,
shot_outcome == "missed" ~ 0),
shot_value = dplyr::case_when(
is.na(shot_outcome) ~ NA_real_,
free_throw == 1 ~ 1,
three_pt == 1 ~ 3,
T ~ 2),
points = dplyr::case_when(
shot_made_numeric == 0 ~ 0,
shot_made_numeric == 1 & free_throw == 1 ~ 1,
shot_made_numeric == 1 & three_pt == 1 ~ 3,
shot_made_numeric == 1 & three_pt == 0 & free_throw == 0 ~ 2)) %>%
dplyr::group_by(date, game_id, poss_before, poss_number) %>%
dplyr::summarise(fgm = sum(shot_outcome == "made" & free_throw == F, na.rm = T),
fga = sum(!is.na(shot_outcome) & free_throw == F),
ftm = sum(shot_outcome == "made" & free_throw == T),
fta = sum(!is.na(shot_outcome) & free_throw == T),
points = sum(points, na.rm = T),
.groups = "drop") %>%
dplyr::group_by(date, game_id, poss_before) %>%
dplyr::summarise(poss = dplyr::n(),
across(fgm:points, sum),
.groups = "drop") %>%
dplyr::mutate(ppp = points/poss,
type = ifelse(poss_before == "Arkansas", "Offense", "Defense"),
color = ifelse(poss_before == "Arkansas", "black", "#003366"),
fill = ifelse(poss_before == "Arkansas", "#001A57", "white")) %>%
ggplot(aes(date, ppp, fill = fill, color = color)) +
geom_line() +
geom_point(aes(size = poss),
pch = 21, stroke = 0.9) +
scale_color_identity() +
scale_fill_identity() +
scale_size_continuous(range = c(0.8, 3.5)) +
labs(title = "Arkansas' offensive and defensive efficiency by game",
subtitle = "2018-19 college basketball season",
x = "Date",
y = "Points per possession",
size = "# of possessions",
caption = "Chart: @jacobhaarala | Data: @gamezoneR")
ark_pbp %>%
dplyr::filter(!is.na(poss_before)) %>%
dplyr::mutate(poss_number = as.numeric(poss_number),
shot_made_numeric = dplyr::case_when(
is.na(shot_outcome) ~ NA_real_,
shot_outcome == "made" ~ 1,
shot_outcome == "missed" ~ 0),
shot_value = dplyr::case_when(
is.na(shot_outcome) ~ NA_real_,
free_throw == 1 ~ 1,
three_pt == 1 ~ 3,
T ~ 2),
points = dplyr::case_when(
shot_made_numeric == 0 ~ 0,
shot_made_numeric == 1 & free_throw == 1 ~ 1,
shot_made_numeric == 1 & three_pt == 1 ~ 3,
shot_made_numeric == 1 & three_pt == 0 & free_throw == 0 ~ 2)) %>%
dplyr::group_by(date, game_id, poss_before, poss_number) %>%
dplyr::summarise(fgm = sum(shot_outcome == "made" & free_throw == F, na.rm = T),
fga = sum(!is.na(shot_outcome) & free_throw == F),
ftm = sum(shot_outcome == "made" & free_throw == T),
fta = sum(!is.na(shot_outcome) & free_throw == T),
points = sum(points, na.rm = T),
.groups = "drop") %>%
dplyr::group_by(date, game_id, poss_before) %>%
dplyr::summarise(poss = dplyr::n(),
across(fgm:points, sum),
.groups = "drop") %>%
dplyr::mutate(ppp = points/poss,
type = ifelse(poss_before == "Arkansas", "Offense", "Defense"),
color = ifelse(poss_before == "Arkansas", "black", "#003366"),
fill = ifelse(poss_before == "Arkansas", "#001A57", "white")) %>%
ggplot(aes(date, ppp, fill = fill, color = color)) +
geom_line() +
geom_point(aes(size = poss),
pch = 21, stroke = 0.9) +
scale_color_identity() +
scale_fill_identity() +
scale_size_continuous(range = c(0.8, 3.5)) +
labs(title = "Arkansas' offensive and defensive efficiency by game",
subtitle = "2021-22 college basketball season",
x = "Date",
y = "Points per possession",
size = "# of possessions",
caption = "Chart: @jacobhaarala | Data: @gamezoneR")
getwd()
setwd("/Users/jacobhaarala/Downloads/Uark_MBB_analytics/")
